{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apolovictor/adm_botecaria/blob/main/Vou/Emoji-Gemma-on-Web/resources/Convert_Gemma_3_270M_to_LiteRT_for_MediaPipe_LLM_Inference_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct3qhBX7rKDC"
      },
      "source": [
        "Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z11nEz9krMGQ"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkOtOq0jDE0c"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "      <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Convert_Gemma_3_270M_to_LiteRT_for_MediaPipe_LLM_Inference_API.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDkoc9st4TYk"
      },
      "source": [
        "# Convert Gemma 3 270M to LiteRT for use with MediaPipe LLM Inference API\n",
        "\n",
        "This notebook converts a Gemma 3 270M for use with the [MediaPipe LLM Inference API](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference), a library that enables inference on mobile devices or in web browsers. The entire process takes about 15 minutes:\n",
        "\n",
        "1. Set up the Colab environment\n",
        "2. Load the model from Hugging Face\n",
        "3. Convert the model with the AI Edge Torch converter\n",
        "4. Package the model with the MediaPipe Task bundler\n",
        "5. Download the model\n",
        "\n",
        "Gemma 3 270M is designed for task-specific fine-tuning and engineered for efficient performance on mobile, web, and edge devices. You can fine-tune your own model using this [notebook](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Fine_tune_Gemma_3_270M_for_emoji_generation.ipynb) and run it in a demo [web app](https://github.com/google-gemini/gemma-cookbook/tree/main/Demos/Emoji-Gemma-on-Web/app-mediapipe) once converted.\n",
        "\n",
        "## Set up development environment\n",
        "\n",
        "The first step is to install packages using pip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oEAwOswpsqeq",
        "outputId": "8a6496bc-22bd-461b-ac75-23d015b2b667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.19.0\n",
            "Uninstalling tensorflow-2.19.0:\n",
            "  Successfully uninstalled tensorflow-2.19.0\n",
            "Collecting tf-nightly==2.21.0.dev20250819\n",
            "  Downloading tf_nightly-2.21.0.dev20250819-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting ai-edge-torch==0.6.0\n",
            "  Downloading ai_edge_torch-0.6.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (5.29.5)\n",
            "Collecting protobuf\n",
            "  Downloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (0.6.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (25.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (3.2.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (1.76.0)\n",
            "Collecting tb-nightly~=2.20.0.a (from tf-nightly==2.21.0.dev20250819)\n",
            "  Downloading tb_nightly-2.20.0a20250717-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting keras-nightly>=3.10.0.dev (from tf-nightly==2.21.0.dev20250819)\n",
            "  Downloading keras_nightly-3.12.0.dev2025100703-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tf-nightly==2.21.0.dev20250819) (0.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch==0.6.0) (1.16.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch==0.6.0) (0.6.2)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch==0.6.0) (1.0.0)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch==0.6.0) (0.3.13)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch==0.6.0) (0.9.0)\n",
            "Requirement already satisfied: torch<2.9.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch==0.6.0) (2.8.0+cu126)\n",
            "Collecting ai-edge-tensorflow==2.21.0.dev20250818 (from ai-edge-torch==0.6.0)\n",
            "  Downloading ai_edge_tensorflow-2.21.0.dev20250818-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting ai-edge-litert==1.4.* (from ai-edge-torch==0.6.0)\n",
            "  Downloading ai_edge_litert-1.4.0-cp312-cp312-manylinux_2_17_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting ai-edge-quantizer==0.3.* (from ai-edge-torch==0.6.0)\n",
            "  Downloading ai_edge_quantizer-0.3.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch==0.6.0) (0.7.2)\n",
            "Collecting torch-xla2>=0.0.1.dev20241201 (from torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch==0.6.0)\n",
            "  Downloading torch_xla2-0.0.1.dev202412041639-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting backports.strenum (from ai-edge-litert==1.4.*->ai-edge-torch==0.6.0)\n",
            "  Downloading backports_strenum-1.2.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert==1.4.*->ai-edge-torch==0.6.0) (4.67.1)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (from ai-edge-quantizer==0.3.*->ai-edge-torch==0.6.0) (4.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tf-nightly==2.21.0.dev20250819) (0.45.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras-nightly>=3.10.0.dev->tf-nightly==2.21.0.dev20250819) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras-nightly>=3.10.0.dev->tf-nightly==2.21.0.dev20250819) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras-nightly>=3.10.0.dev->tf-nightly==2.21.0.dev20250819) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tf-nightly==2.21.0.dev20250819) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tf-nightly==2.21.0.dev20250819) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tf-nightly==2.21.0.dev20250819) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tf-nightly==2.21.0.dev20250819) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tb-nightly~=2.20.0.a->tf-nightly==2.21.0.dev20250819) (3.9)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tb-nightly~=2.20.0.a->tf-nightly==2.21.0.dev20250819) (11.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tb-nightly~=2.20.0.a->tf-nightly==2.21.0.dev20250819) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tb-nightly~=2.20.0.a->tf-nightly==2.21.0.dev20250819) (3.1.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (3.4.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch==0.6.0) (8.4.2)\n",
            "Requirement already satisfied: jaxlib<=0.7.2,>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from jax->ai-edge-torch==0.6.0) (0.7.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<2.9.0,>=2.4.0->ai-edge-torch==0.6.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tb-nightly~=2.20.0.a->tf-nightly==2.21.0.dev20250819) (3.0.3)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch==0.6.0) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch==0.6.0) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest->torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch==0.6.0) (2.19.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras-nightly>=3.10.0.dev->tf-nightly==2.21.0.dev20250819) (4.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nightly>=3.10.0.dev->tf-nightly==2.21.0.dev20250819) (0.1.2)\n",
            "Downloading tf_nightly-2.21.0.dev20250819-cp312-cp312-manylinux_2_27_x86_64.whl (532.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.3/532.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ai_edge_torch-0.6.0-py3-none-any.whl (439 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.4/439.4 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ai_edge_litert-1.4.0-cp312-cp312-manylinux_2_17_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ai_edge_quantizer-0.3.0-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ai_edge_tensorflow-2.21.0.dev20250818-cp312-cp312-manylinux_2_27_x86_64.whl (254.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.2/254.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_nightly-3.12.0.dev2025100703-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tb_nightly-2.20.0a20250717-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_xla2-0.0.1.dev202412041639-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backports_strenum-1.2.8-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: protobuf, backports.strenum, tb-nightly, ai-edge-litert, keras-nightly, torch-xla2, tf-nightly, ai-edge-tensorflow, ai-edge-quantizer, ai-edge-torch\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, which is not installed.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ai-edge-litert-1.4.0 ai-edge-quantizer-0.3.0 ai-edge-tensorflow-2.21.0.dev20250818 ai-edge-torch-0.6.0 backports.strenum-1.2.8 keras-nightly-3.12.0.dev2025100703 protobuf-6.33.0 tb-nightly-2.20.0a20250717 tf-nightly-2.21.0.dev20250819 torch-xla2-0.0.1.dev202412041639\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "a2864a5bd314449fbdaf1d4babc0332e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (0.7.2)\n",
            "Collecting jax\n",
            "  Downloading jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (0.7.2)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax) (0.5.3)\n",
            "Requirement already satisfied: numpy>=2.0 in /usr/local/lib/python3.12/dist-packages (from jax) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax) (1.16.3)\n",
            "Downloading jax-0.8.0-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl (79.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.7.2\n",
            "    Uninstalling jaxlib-0.7.2:\n",
            "      Successfully uninstalled jaxlib-0.7.2\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.7.2\n",
            "    Uninstalling jax-0.7.2:\n",
            "      Successfully uninstalled jax-0.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.8.0 jaxlib-0.8.0\n"
          ]
        }
      ],
      "source": [
        "%pip uninstall -y tensorflow\n",
        "%pip install -U tf-nightly==2.21.0.dev20250819 ai-edge-torch==0.6.0 protobuf transformers\n",
        "%pip install -U jax jaxlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8U1zoAY-4g4"
      },
      "source": [
        "Restart the session runtime to ensure you're using the newly installed packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtibyxZmZjHG"
      },
      "source": [
        "## Load the model\n",
        "To access models on Hugging Face, provide your [Access Token](https://huggingface.co/settings/tokens). You can store it as a Colab secret in the left toolbar by specifying `HF_TOKEN` as the 'Name' and adding your unique token as the 'Value'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sCQO6oy41gZs",
        "outputId": "75e4db96-4c69-4267-8c95-7f79e2c166fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'repo_id' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4063138895.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/content/{model_name}\"\u001b[0m                      \u001b[0;31m# Path to save resized model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# Load the tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'repo_id' is not defined"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "# --- SUA ADAPTAÇÃO AQUI ---\n",
        "# Em vez de baixar, aponte diretamente para a sua pasta de checkpoint local.\n",
        "# Certifique-se de que a pasta 'gemma-fine-tuned' foi carregada no seu ambiente Colab.\n",
        "local_model_path = 'checkpoint-60'\n",
        "\n",
        "# Nome que daremos ao modelo convertido\n",
        "model_author = \"vou\"                                         #@param {type:\"string\"}\n",
        "model_name = 'gemma-3-270m-finetuned'\n",
        "# -----------------------------\n",
        "repo_id = f\"{model_author}/{model_name}\"                  # Model to convert\n",
        "save_path = f\"/content/{model_name}\"                      # Path to save resized model\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(repo_id)     # Load the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)        # Load the tokenizer\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk2ni3i11LRY"
      },
      "source": [
        "Specify the Hugging Face repo ID of the model to convert. It'll be saved to your Colab files for conversion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4p7yHYZ1BKh"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_author = \"\"                                         #@param {type:\"string\"}\n",
        "model_name = \"myemoji-gemma-3-270m-it\"                    #@param {type:\"string\"}\n",
        "\n",
        "repo_id = f\"{model_author}/{model_name}\"                  # Model to convert\n",
        "save_path = f\"/content/{model_name}\"                      # Path to save resized model\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(repo_id)     # Load the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)        # Load the tokenizer\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTvHIgTT-KzZ"
      },
      "source": [
        "## Convert the model\n",
        "Convert and quantize the model using the [AI Edge Torch](https://github.com/google-ai-edge/ai-edge-torch) converter. You can adjust the conversions parameters based on your task's requirements:\n",
        "\n",
        "* `prefill_seq_len`: maximum length of supported input\n",
        "* `kv_cache_max_len`: maximum of prefill + decode context length\n",
        "* `quantize`: the quantization scheme. 8-bit integer quantization (INT8) is good for web environments\n",
        "\n",
        "This takes about 10 minutes. The .tflite model will be saved temporarily to your Colab files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vzTdvia1n7E"
      },
      "outputs": [],
      "source": [
        "from ai_edge_torch.generative.examples.gemma3 import gemma3\n",
        "from ai_edge_torch.generative.utilities import converter\n",
        "from ai_edge_torch.generative.utilities.export_config import ExportConfig\n",
        "from ai_edge_torch.generative.layers import kv_cache\n",
        "\n",
        "# Get model, set export settings, and convert to .tflite\n",
        "pytorch_model = gemma3.build_model_270m(save_path)\n",
        "export_config = ExportConfig()\n",
        "export_config.kvcache_layout = kv_cache.KV_LAYOUT_TRANSPOSED\n",
        "export_config.mask_as_input = True\n",
        "converter.convert_to_tflite(\n",
        "    pytorch_model,\n",
        "    output_path=\"/content\",\n",
        "    output_name_prefix=model_name,\n",
        "    prefill_seq_len=128,\n",
        "    kv_cache_max_len=512,\n",
        "    quantize=\"dynamic_int8\",\n",
        "    export_config=export_config,\n",
        ")\n",
        "\n",
        "print (f\"Model converted to .tflite and saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqFSmQxi-rCF"
      },
      "source": [
        "## Create a MediaPipe Task Bundle\n",
        "\n",
        "A MediaPipe Task file (.task) bundles the original model tokenizer, the LiteRT model (.tflite), and additional metadata needed to run end-to-end inference with the MediaPipe LLM Inference API.\n",
        "\n",
        "To use the bundler, install the MediaPipe PyPI package (>0.10.14) in this step as it comes with its own set of dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrss72uJ-obD"
      },
      "outputs": [],
      "source": [
        "%pip install mediapipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "essOk3jeDmOV"
      },
      "source": [
        "Do a fresh install of `protobuf` and `tensorflow` and restart the Colab runtime to get the latest versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LhWaLPOiUTy"
      },
      "outputs": [],
      "source": [
        "%pip uninstall protobuf -y && pip install protobuf\n",
        "%pip uninstall tensorflow -y -q && pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHC7dEhR7gFJ"
      },
      "source": [
        "Now, you'll configure and create the Task bundle:\n",
        "\n",
        "1. Update `tflite_model` to point to the newly converted .tflite model in your Colab files\n",
        "2. Update `tokenizer_model` to point to the tokenizer.model that was downloaded from Hugging Face Hub.\n",
        "3. Name your .task file in the `output_filename`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OxNmqgF-VO0"
      },
      "outputs": [],
      "source": [
        "from mediapipe.tasks.python.genai import bundler\n",
        "\n",
        "config = bundler.BundleConfig(\n",
        "    tflite_model=\"/content/myemoji-gemma-3-270m-it_q8_ekv512.tflite\",     # Point to your converted .tflite model\n",
        "    tokenizer_model=\"/content/myemoji-gemma-3-270m-it/tokenizer.model\",   # Point to the downloaded model's tokenizer.model file\n",
        "    start_token=\"<bos>\",\n",
        "    stop_tokens=[\"<eos>\", \"<end_of_turn>\"],\n",
        "    output_filename=\"/content/myemoji-gemma-3-270m-it.task\",              # Specify the final model filename\n",
        "    prompt_prefix=\"<start_of_turn>user\\n\",\n",
        "    prompt_suffix=\"<end_of_turn>\\n<start_of_turn>model\\n\",\n",
        ")\n",
        "bundler.create_bundle(config)\n",
        "\n",
        "print(f\"Model .task bundle saved to {config.output_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXtdT_2E5vrO"
      },
      "source": [
        "## Download & run your model on-device\n",
        "\n",
        "Your model is now ready for on-device inference using the MediaPipe LLM Inference API!\n",
        "\n",
        "Download the .task file from your Colab environment to use it in your projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdfSriJW5vNk"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(config.output_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh1fQTQrcpUh"
      },
      "source": [
        "Try it in the [emoji generation web app](https://github.com/google-gemini/gemma-cookbook/tree/main/Demos/Emoji-Gemma-on-Web/app-mediapipe) which runs the model directly in the browser. You can also explore [documentation](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference) for building cross-platform mobile and web apps."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Convert_Gemma_3_270M_to_LiteRT_for_MediaPipe_LLM_Inference_API.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}